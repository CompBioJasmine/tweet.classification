{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_table('~/Downloads/INFO_4604/AwarenessVsInfection_month.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Twitter_Id</th>\n",
       "      <th>Twitter_content</th>\n",
       "      <th>Time</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4063561008</td>\n",
       "      <td>you gotta be kidding me .. everyone around me ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4063912301</td>\n",
       "      <td>getting my flu shot then babysitting .</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4064279613</td>\n",
       "      <td>my arm is getting sore from that damn flu shot...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4066630902</td>\n",
       "      <td>seems like these bird profile pics are spreadi...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4066949107</td>\n",
       "      <td>i think i'm in the process of getting the flu ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Twitter_Id                                    Twitter_content  Time  intent\n",
       "0  4063561008  you gotta be kidding me .. everyone around me ...     9       0\n",
       "1  4063912301             getting my flu shot then babysitting .     9       1\n",
       "2  4064279613  my arm is getting sore from that damn flu shot...     9       1\n",
       "3  4066630902  seems like these bird profile pics are spreadi...     9       1\n",
       "4  4066949107  i think i'm in the process of getting the flu ...     9       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = df.loc[:,['Twitter_content','intent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Twitter_content</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you gotta be kidding me .. everyone around me ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>getting my flu shot then babysitting .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my arm is getting sore from that damn flu shot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seems like these bird profile pics are spreadi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i think i'm in the process of getting the flu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Twitter_content  intent\n",
       "0  you gotta be kidding me .. everyone around me ...       0\n",
       "1             getting my flu shot then babysitting .       1\n",
       "2  my arm is getting sore from that damn flu shot...       1\n",
       "3  seems like these bird profile pics are spreadi...       1\n",
       "4  i think i'm in the process of getting the flu ...       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = new_df.rename(index=str, columns={\"Twitter_content\": \"Text\", \"intent\": \"Illness\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Illness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you gotta be kidding me .. everyone around me ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>getting my flu shot then babysitting .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my arm is getting sore from that damn flu shot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seems like these bird profile pics are spreadi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i think i'm in the process of getting the flu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Illness\n",
       "0  you gotta be kidding me .. everyone around me ...        0\n",
       "1             getting my flu shot then babysitting .        1\n",
       "2  my arm is getting sore from that damn flu shot...        1\n",
       "3  seems like these bird profile pics are spreadi...        1\n",
       "4  i think i'm in the process of getting the flu ...        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "text_train, text_test, Y_train, Y_test = train_test_split(df2['Text'], df2['Illness'], test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def ngrams(tokens, n):\n",
    "    output = []\n",
    "    for i in range(n-1, len(tokens)):\n",
    "        ngram = ' '.join(tokens[i-n+1:i+1])\n",
    "        output.append(ngram)\n",
    "    return output\n",
    "\n",
    "def features(text, ngram_range=(1,1)):\n",
    "    text = text.lower()      # make the string lowercase\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)     # remove consecutive characters that are repeated more than twice\n",
    "    \n",
    "    features_in_text = []   # running list of all features in this instance (can be repeated)\n",
    "    \n",
    "    # treat alphanumeric characters as word tokens (removing anything else),\n",
    "    # and extract all n-grams of length n specified by ngram_range\n",
    "    \n",
    "    text_alphanum = re.sub('[^a-z0-9]', ' ', text)\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        features_in_text += ngrams(text_alphanum.split(), n)\n",
    "    \n",
    "    # now treat punctuation as word tokens, and get their counts (only unigrams)\n",
    "    \n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    features_in_text += ngrams(text_punc.split(), 1)\n",
    "    \n",
    "    # 'Counter' converts a list into a dictionary whose keys are the list elements \n",
    "    #  and the values are the number of times each element appeared in the list\n",
    "    \n",
    "    return Counter(features_in_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vect = DictVectorizer()\n",
    "X_train = vect.fit_transform(features(d, ngram_range=(1,3)) for d in text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter settings: {'C': 0.5}\n",
      "Validation accuracy: 0.803517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# this defines the classifier we will use -- don't change this variable\n",
    "\n",
    "base_classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=123)\n",
    "\n",
    "# these are the C values we will compare -- don't change this variable\n",
    "\n",
    "params = [{'C': [0.01, 0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 50.0, 100.0]}]\n",
    "\n",
    "# this performs 5-fold cross-validation with the above classifier and parameter options\n",
    "\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best parameter settings:\", gs_classifier.best_params_)\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.803517\n",
      "Validation accuracy: 0.786697\n",
      "Validation accuracy: 0.736239\n",
      "Validation accuracy: 0.804281\n",
      "Validation accuracy: 0.808104\n",
      "Validation accuracy: 0.785168\n"
     ]
    }
   ],
   "source": [
    "X_train = vect.fit_transform(features(d, ngram_range=(1,1)) for d in text_train)\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train, Y_train)\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "\n",
    "X_train = vect.fit_transform(features(d, ngram_range=(2,2)) for d in text_train)\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train, Y_train)\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "\n",
    "X_train = vect.fit_transform(features(d, ngram_range=(3,3)) for d in text_train)\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train, Y_train)\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "\n",
    "X_train = vect.fit_transform(features(d, ngram_range=(1,2)) for d in text_train)\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train, Y_train)\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "\n",
    "X_train = vect.fit_transform(features(d, ngram_range=(1,3)) for d in text_train)\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train, Y_train)\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "\n",
    "X_train = vect.fit_transform(features(d, ngram_range=(2,3)) for d in text_train)\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train, Y_train)\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.808104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.76425856, 0.82442748, 0.81609195, 0.81226054, 0.81226054])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vect.fit_transform(features(d, ngram_range=(1,3)) for d in text_train)\n",
    "X_test = vect.fit_transform(features(d, ngram_range=(1,3)) for d in text_test)\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train, Y_train)\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "\n",
    "scores = cross_val_score(gs_classifier, X_train, Y_train, cv=5)\n",
    "scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Percentile vs. Accuracy for Ngram Range (1,3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRZT0RURSRElfRCRFlPRFRFLk/wD2Bv4xm8PnDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1104ad940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "percentiles = [1,2,5,10,20,30,40,50,60,70,80,90,100]\n",
    "accuracies = []\n",
    "\n",
    "for p in range(len(percentiles)):\n",
    "    selection = SelectPercentile(percentile=percentiles[p], score_func=chi2)\n",
    "    X_train_selected = selection.fit_transform(X_train, Y_train)\n",
    "    gs_classifier.fit(X_train_selected, Y_train)\n",
    "    accuracies.append(gs_classifier.best_score_)\n",
    "\n",
    "print(\"\\n\\nPercentile vs. Accuracy for Ngram Range (1,3)\")\n",
    "plt.plot(percentiles, accuracies, 'b--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.844801\n",
      "Test accuracy: 0.813073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.76425856, 0.82442748, 0.81609195, 0.81226054, 0.81226054])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final = vect.fit_transform(features(d, ngram_range=(1,3)) for d in text_train)\n",
    "X_test_final = vect.transform(features(d, ngram_range=(1,3)) for d in text_test)\n",
    "\n",
    "selection = SelectPercentile(percentile=85, score_func=chi2)\n",
    "X_train_final = selection.fit_transform(X_train_final, Y_train)\n",
    "X_test_final = selection.transform(X_test_final)\n",
    "\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train_final, Y_train)\n",
    "\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, gs_classifier.predict(X_test_final)))\n",
    "\n",
    "scores = cross_val_score(gs_classifier, X_train, Y_train, cv=5)\n",
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgrams(tokens, n):\n",
    "    output = []\n",
    "    for i in range(n-1, len(tokens)):\n",
    "        str1 = ''\n",
    "        for j in range(n-2):\n",
    "            str1 = str1+' * '\n",
    "        skipgram = str(tokens[i-n+1])+str1+str(tokens[i])\n",
    "        output.append(skipgram)\n",
    "    return output\n",
    "\n",
    "\n",
    "def skipgramfeatures(text, ngram_range=(3,4)):\n",
    "    text = text.lower()      # make the string lowercase\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)     # remove consecutive characters that are repeated more than twice\n",
    "    \n",
    "    features_in_text = []   # running list of all features in this instance (can be repeated)\n",
    "    \n",
    "    # treat alphanumeric characters as word tokens (removing anything else),\n",
    "    # and extract all n-grams of length n specified by ngram_range\n",
    "    \n",
    "    text_alphanum = re.sub('[^a-z0-9]', ' ', text)\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        features_in_text += skipgrams(text_alphanum.split(), n)\n",
    "    \n",
    "    \n",
    "    # 'Counter' converts a list into a dictionary whose keys are the list elements \n",
    "    #  and the values are the number of times each element appeared in the list\n",
    "    \n",
    "    return Counter(features_in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the * is', 'water * very', 'is * cold']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ['the','water','is','very','cold']\n",
    "\n",
    "skipgrams(sample,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.829511\n",
      "Test accuracy: 0.748853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.76425856, 0.82442748, 0.81609195, 0.81226054, 0.81226054])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final = vect.fit_transform(skipgramfeatures(d) for d in text_train)\n",
    "X_test_final = vect.transform(skipgramfeatures(d) for d in text_test)\n",
    "\n",
    "selection = SelectPercentile(percentile=85, score_func=chi2)\n",
    "X_train_final = selection.fit_transform(X_train_final, Y_train)\n",
    "X_test_final = selection.transform(X_test_final)\n",
    "\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train_final, Y_train)\n",
    "\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, gs_classifier.predict(X_test_final)))\n",
    "\n",
    "scores = cross_val_score(gs_classifier, X_train, Y_train, cv=5)\n",
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.844801\n",
      "Test accuracy: 0.813073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.76425856, 0.82442748, 0.81609195, 0.81226054, 0.81226054])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final = vect.fit_transform(features(d, ngram_range=(1,3)) for d in text_train)\n",
    "X_test_final = vect.transform(features(d, ngram_range=(1,3)) for d in text_test)\n",
    "\n",
    "selection = SelectPercentile(percentile=85, score_func=chi2)\n",
    "X_train_final = selection.fit_transform(X_train_final, Y_train)\n",
    "X_test_final = selection.transform(X_test_final)\n",
    "\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train_final, Y_train)\n",
    "\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, gs_classifier.predict(X_test_final)))\n",
    "\n",
    "scores = cross_val_score(gs_classifier, X_train, Y_train, cv=5)\n",
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features(text, ngram_range=(1,3)):\n",
    "    text = text.lower()      # make the string lowercase\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)     # remove consecutive characters that are repeated more than twice\n",
    "    \n",
    "    features_in_text = []   # running list of all features in this instance (can be repeated)\n",
    "    \n",
    "    # treat alphanumeric characters as word tokens (removing anything else),\n",
    "    # and extract all n-grams of length n specified by ngram_range\n",
    "    \n",
    "    text_alphanum = re.sub('[^a-z0-9]', ' ', text)\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        features_in_text += ngrams(text_alphanum.split(), n)\n",
    "    \n",
    "    for n in range(3,5):\n",
    "        features_in_text += skipgrams(text_alphanum.split(), n)\n",
    "        \n",
    "    \n",
    "    # now treat punctuation as word tokens, and get their counts (only unigrams)\n",
    "    \n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    features_in_text += ngrams(text_punc.split(), 1)\n",
    "    \n",
    "    \n",
    "    # 'Counter' converts a list into a dictionary whose keys are the list elements \n",
    "    #  and the values are the number of times each element appeared in the list\n",
    "    \n",
    "    return Counter(features_in_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.853211\n",
      "Test accuracy: 0.815367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.76425856, 0.82442748, 0.81609195, 0.81226054, 0.81226054])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final = vect.fit_transform(features(d, ngram_range=(1,3)) for d in text_train)\n",
    "X_test_final = vect.transform(features(d, ngram_range=(1,3)) for d in text_test)\n",
    "\n",
    "selection = SelectPercentile(percentile=85, score_func=chi2)\n",
    "X_train_final = selection.fit_transform(X_train_final, Y_train)\n",
    "X_test_final = selection.transform(X_test_final)\n",
    "\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train_final, Y_train)\n",
    "\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, gs_classifier.predict(X_test_final)))\n",
    "\n",
    "scores = cross_val_score(gs_classifier, X_train, Y_train, cv=5)\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very slight improvement with skipgrams here it appears though hard to say exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv('http://cmci.colorado.edu/classes/INFO-4604/data/sentiment-words.csv', header=None,encoding = \"ISO-8859-1\")\n",
    "\n",
    "# The two variables, positive_words and negative_words, are sets that contain the positive/negative words\n",
    "\n",
    "positive_words = sentiment_df.loc[sentiment_df[0] == 'positive']\n",
    "positive_words = set(positive_words.iloc[0:, 1].values)\n",
    "\n",
    "negative_words = sentiment_df.loc[sentiment_df[0] == 'negative']\n",
    "negative_words = set(negative_words.iloc[0:, 1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment(ngrams):\n",
    "    output = []\n",
    "    for ngram in ngrams:\n",
    "        for word in ngram.split():\n",
    "            s = re.compile(word)\n",
    "            if word in positive_words:\n",
    "                replace = s.sub('POS',ngram)\n",
    "                output.append(replace)\n",
    "            if word in negative_words:\n",
    "                replace = s.sub('NEG',ngram)\n",
    "                output.append(replace)\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the * is', 'water * very', 'is * cold']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ['the','water','is','very','cold']\n",
    "\n",
    "skipgrams(sample,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the water is', 'water is very', 'is very cold']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = ngrams(sample,3)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is very NEG']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features(text, ngram_range=(1,3)):\n",
    "    text = text.lower()      # make the string lowercase\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)     # remove consecutive characters that are repeated more than twice\n",
    "    \n",
    "    features_in_text = []   # running list of all features in this instance (can be repeated)\n",
    "    \n",
    "    # treat alphanumeric characters as word tokens (removing anything else),\n",
    "    # and extract all n-grams of length n specified by ngram_range\n",
    "    \n",
    "    text_alphanum = re.sub('[^a-z0-9]', ' ', text)\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        features_in_text += ngrams(text_alphanum.split(), n)\n",
    "        features_in_text += sentiment(ngrams(text_alphanum.split(), n))\n",
    "    \n",
    "    for n in range(3,5):\n",
    "        features_in_text += skipgrams(text_alphanum.split(), n)\n",
    "        \n",
    "    # now treat punctuation as word tokens, and get their counts (only unigrams)\n",
    "    \n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    features_in_text += ngrams(text_punc.split(), 1)\n",
    "    \n",
    "    \n",
    "    # 'Counter' converts a list into a dictionary whose keys are the list elements \n",
    "    #  and the values are the number of times each element appeared in the list\n",
    "    \n",
    "    return Counter(features_in_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.850917\n",
      "Test accuracy: 0.814220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.76425856, 0.82442748, 0.81609195, 0.81226054, 0.81226054])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final = vect.fit_transform(features(d, ngram_range=(1,3)) for d in text_train)\n",
    "X_test_final = vect.transform(features(d, ngram_range=(1,3)) for d in text_test)\n",
    "\n",
    "selection = SelectPercentile(percentile=85, score_func=chi2)\n",
    "X_train_final = selection.fit_transform(X_train_final, Y_train)\n",
    "X_test_final = selection.transform(X_test_final)\n",
    "\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train_final, Y_train)\n",
    "\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, gs_classifier.predict(X_test_final)))\n",
    "\n",
    "scores = cross_val_score(gs_classifier, X_train, Y_train, cv=5)\n",
    "scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def pairs(words):\n",
    "    ret = itertools.combinations(words,2)\n",
    "    return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'water')\n",
      "('the', 'is')\n",
      "('the', 'very')\n",
      "('the', 'cold')\n",
      "('water', 'is')\n",
      "('water', 'very')\n",
      "('water', 'cold')\n",
      "('is', 'very')\n",
      "('is', 'cold')\n",
      "('very', 'cold')\n"
     ]
    }
   ],
   "source": [
    "for pair in pairs(sample):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features(text, ngram_range=(1,3)):\n",
    "    text = text.lower()      # make the string lowercase\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)     # remove consecutive characters that are repeated more than twice\n",
    "    \n",
    "    features_in_text = []   # running list of all features in this instance (can be repeated)\n",
    "    \n",
    "    # treat alphanumeric characters as word tokens (removing anything else),\n",
    "    # and extract all n-grams of length n specified by ngram_range\n",
    "    \n",
    "    text_alphanum = re.sub('[^a-z0-9]', ' ', text)\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        features_in_text += ngrams(text_alphanum.split(), n)\n",
    "        features_in_text += sentiment(ngrams(text_alphanum.split(), n))\n",
    "    \n",
    "    for n in range(3,5):\n",
    "        features_in_text += skipgrams(text_alphanum.split(), n)\n",
    "      \n",
    "    features_in_text += str(pairs(text_alphanum.split()))\n",
    "    # now treat punctuation as word tokens, and get their counts (only unigrams)\n",
    "    \n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    features_in_text += ngrams(text_punc.split(), 1)\n",
    "    \n",
    "   \n",
    "    # 'Counter' converts a list into a dictionary whose keys are the list elements \n",
    "    #  and the values are the number of times each element appeared in the list\n",
    "    \n",
    "    return Counter(features_in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.850153\n",
      "Test accuracy: 0.822248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.76425856, 0.82442748, 0.81609195, 0.81226054, 0.81226054])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final = vect.fit_transform(features(d, ngram_range=(1,3)) for d in text_train)\n",
    "X_test_final = vect.transform(features(d, ngram_range=(1,3)) for d in text_test)\n",
    "\n",
    "selection = SelectPercentile(percentile=85, score_func=chi2)\n",
    "X_train_final = selection.fit_transform(X_train_final, Y_train)\n",
    "X_test_final = selection.transform(X_test_final)\n",
    "\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train_final, Y_train)\n",
    "\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, gs_classifier.predict(X_test_final)))\n",
    "\n",
    "scores = cross_val_score(gs_classifier, X_train, Y_train, cv=5)\n",
    "scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features(text, ngram_range=(1,3)):\n",
    "    text = text.lower()      # make the string lowercase\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)     # remove consecutive characters that are repeated more than twice\n",
    "    \n",
    "    features_in_text = []   # running list of all features in this instance (can be repeated)\n",
    "    \n",
    "    # treat alphanumeric characters as word tokens (removing anything else),\n",
    "    # and extract all n-grams of length n specified by ngram_range\n",
    "    \n",
    "    text_alphanum = re.sub('[^a-z0-9]', ' ', text)\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        features_in_text += ngrams(text_alphanum.split(), n)\n",
    "        #remove sentiment\n",
    "    \n",
    "    for n in range(3,5):\n",
    "        features_in_text += skipgrams(text_alphanum.split(), n)\n",
    "      \n",
    "    features_in_text += str(pairs(text_alphanum.split()))\n",
    "    # now treat punctuation as word tokens, and get their counts (only unigrams)\n",
    "    \n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    features_in_text += ngrams(text_punc.split(), 1)\n",
    "    \n",
    "   \n",
    "    # 'Counter' converts a list into a dictionary whose keys are the list elements \n",
    "    #  and the values are the number of times each element appeared in the list\n",
    "    \n",
    "    return Counter(features_in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.846330\n",
      "Test accuracy: 0.811927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.76425856, 0.82442748, 0.81609195, 0.81226054, 0.81226054])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final = vect.fit_transform(features(d, ngram_range=(1,3)) for d in text_train)\n",
    "X_test_final = vect.transform(features(d, ngram_range=(1,3)) for d in text_test)\n",
    "\n",
    "selection = SelectPercentile(percentile=85, score_func=chi2)\n",
    "X_train_final = selection.fit_transform(X_train_final, Y_train)\n",
    "X_test_final = selection.transform(X_test_final)\n",
    "\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=5)\n",
    "gs_classifier.fit(X_train_final, Y_train)\n",
    "\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, gs_classifier.predict(X_test_final)))\n",
    "\n",
    "scores = cross_val_score(gs_classifier, X_train, Y_train, cv=5)\n",
    "scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
